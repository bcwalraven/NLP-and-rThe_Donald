{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the effectiveness of our approach, we will duplicate it on completely separate subreddits to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pushshift(subreddit, content, start_date, end_date, search_range):\n",
    "    if 24 % search_range != 0:\n",
    "        print('not a valid search_range')\n",
    "        return None\n",
    "    \n",
    "    # setting up number of inital conditions\n",
    "    hours = ((end_date-start_date).days + 1) * (24//search_range)\n",
    "    before = (dt.date.today()-end_date).days*24\n",
    "    after = before + search_range\n",
    "    completed_hours = 0\n",
    "    completed_days = 0\n",
    "    \n",
    "    dict_list = []\n",
    "\n",
    "    for _ in range(hours):\n",
    "        # sets up url with the specified subreddit and next hour block\n",
    "        url = f'https://api.pushshift.io/reddit/search/{content}/?subreddit={subreddit}&size=500&before={str(before)}h&after={str(after)}h'\n",
    "        res = requests.get(url)\n",
    "        \n",
    "        # checks for a valid connection\n",
    "        while res.status_code != 200:\n",
    "            print(res.status_code)\n",
    "            res = requests.get(url)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        the_json = res.json()\n",
    "        dict_list.extend(the_json['data'])\n",
    "        \n",
    "        before += search_range\n",
    "        after += search_range\n",
    "        completed_hours += search_range\n",
    "        \n",
    "        if completed_hours % 240 == 0:\n",
    "            completed_days += 10\n",
    "            print(f'Days Complete: {completed_days}')\n",
    "        time.sleep(1)\n",
    "\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_to_dataframe(data):\n",
    "    posts = []\n",
    "    for dct in data:\n",
    "        post_dict = {}\n",
    "        post_dict['subreddit'] = dct['subreddit']\n",
    "        post_dict['title'] = dct['title']\n",
    "        post_dict['author'] = dct['author']\n",
    "        post_dict['score'] = dct['score']\n",
    "        post_dict['domain'] = dct['domain']\n",
    "        post_dict['source_url'] = dct['url']\n",
    "        posts.append(post_dict)\n",
    "\n",
    "    return pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Complete: 10\n",
      "Days Complete: 20\n",
      "Days Complete: 30\n",
      "Days Complete: 40\n",
      "Days Complete: 50\n",
      "Days Complete: 60\n",
      "Days Complete: 70\n",
      "Days Complete: 80\n",
      "Days Complete: 90\n"
     ]
    }
   ],
   "source": [
    "zelda_scrape = scrape_pushshift(subreddit = 'zelda', \n",
    "                                content = 'submission', \n",
    "                                start_date = dt.date(2018, 9, 1), \n",
    "                                end_date = dt.date(2018, 11, 30),\n",
    "                                search_range = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>score</th>\n",
       "      <th>source_url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zatchquill</td>\n",
       "      <td>self.zelda</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/zelda/comments/a1ob7g...</td>\n",
       "      <td>zelda</td>\n",
       "      <td>Has a Zelda game inspired hobbies and careers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anilxe</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>1</td>\n",
       "      <td>https://i.imgur.com/EN2gFLQ.gifv</td>\n",
       "      <td>zelda</td>\n",
       "      <td>Hopping off these and sliding off into the wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QuantumFighter</td>\n",
       "      <td>self.zelda</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/zelda/comments/a1oqyb...</td>\n",
       "      <td>zelda</td>\n",
       "      <td>I'm playing through all of the main series LOZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Viathan1108</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>1</td>\n",
       "      <td>https://i.redd.it/a635rxt73e121.jpg</td>\n",
       "      <td>zelda</td>\n",
       "      <td>My first tat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>landyoop</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/LookAtT74424605/status/106...</td>\n",
       "      <td>zelda</td>\n",
       "      <td>Pretty cool. You can get the The Legend of Zel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author       domain  score  \\\n",
       "0      zatchquill   self.zelda      1   \n",
       "1          Anilxe  i.imgur.com      1   \n",
       "2  QuantumFighter   self.zelda      1   \n",
       "3     Viathan1108    i.redd.it      1   \n",
       "4        landyoop  twitter.com      1   \n",
       "\n",
       "                                          source_url subreddit  \\\n",
       "0  https://www.reddit.com/r/zelda/comments/a1ob7g...     zelda   \n",
       "1                   https://i.imgur.com/EN2gFLQ.gifv     zelda   \n",
       "2  https://www.reddit.com/r/zelda/comments/a1oqyb...     zelda   \n",
       "3                https://i.redd.it/a635rxt73e121.jpg     zelda   \n",
       "4  https://twitter.com/LookAtT74424605/status/106...     zelda   \n",
       "\n",
       "                                               title  \n",
       "0  Has a Zelda game inspired hobbies and careers ...  \n",
       "1  Hopping off these and sliding off into the wat...  \n",
       "2  I'm playing through all of the main series LOZ...  \n",
       "3                                      My first tat.  \n",
       "4  Pretty cool. You can get the The Legend of Zel...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zelda_data = scrape_to_dataframe(zelda_scrape)\n",
    "zelda_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Complete: 10\n",
      "Days Complete: 20\n",
      "Days Complete: 30\n"
     ]
    }
   ],
   "source": [
    "bitcoin_scrape = scrape_pushshift(subreddit = 'bitcoin', \n",
    "                                  content = 'submission', \n",
    "                                  start_date = dt.date(2018, 11, 1), \n",
    "                                  end_date = dt.date(2018, 11, 30),\n",
    "                                  search_range = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>score</th>\n",
       "      <th>source_url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jpoc1</td>\n",
       "      <td>self.Bitcoin</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/a1ob...</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>Bitcoin’s Crash and Post-Thanksgiving Market P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dubblies</td>\n",
       "      <td>self.Bitcoin</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/a1od...</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>Bitcoin recovery method - anything in the works?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>herzmeister</td>\n",
       "      <td>medium.com</td>\n",
       "      <td>1</td>\n",
       "      <td>https://medium.com/@kaykurokawa/the-nigerian-n...</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>The Nigerian Nakamoto Scam – Kay Kurokawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>content404</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>1</td>\n",
       "      <td>https://i.redd.it/0s07eorsvd121.png</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>This is why short term volatility doesn't faze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CumiasTheBeesKnees</td>\n",
       "      <td>imgflip.com</td>\n",
       "      <td>1</td>\n",
       "      <td>https://imgflip.com/i/2nsnss</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>When withdrawing on Bitfinex.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author        domain  score  \\\n",
       "0               jpoc1  self.Bitcoin      1   \n",
       "1            dubblies  self.Bitcoin      1   \n",
       "2         herzmeister    medium.com      1   \n",
       "3          content404     i.redd.it      1   \n",
       "4  CumiasTheBeesKnees   imgflip.com      1   \n",
       "\n",
       "                                          source_url subreddit  \\\n",
       "0  https://www.reddit.com/r/Bitcoin/comments/a1ob...   Bitcoin   \n",
       "1  https://www.reddit.com/r/Bitcoin/comments/a1od...   Bitcoin   \n",
       "2  https://medium.com/@kaykurokawa/the-nigerian-n...   Bitcoin   \n",
       "3                https://i.redd.it/0s07eorsvd121.png   Bitcoin   \n",
       "4                       https://imgflip.com/i/2nsnss   Bitcoin   \n",
       "\n",
       "                                               title  \n",
       "0  Bitcoin’s Crash and Post-Thanksgiving Market P...  \n",
       "1   Bitcoin recovery method - anything in the works?  \n",
       "2          The Nigerian Nakamoto Scam – Kay Kurokawa  \n",
       "3  This is why short term volatility doesn't faze...  \n",
       "4                      When withdrawing on Bitfinex.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_data = scrape_to_dataframe(bitcoin_scrape)\n",
    "bitcoin_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([zelda_data, bitcoin_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removal_mask = (combined_data.title == '[removed]') | (combined_data.title == '[deleted]') \n",
    "combined_data = combined_data.loc[~removal_mask, :]\n",
    "combined_data.dropna(inplace=True)\n",
    "combined_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13176, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup      \n",
    "from nltk.corpus import stopwords\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    text = re.sub(r'https:[^\\s]+', repl='', string=raw_text)\n",
    "    text = re.sub(r'http:[^\\s]+', repl='', string=text)\n",
    "    text = BeautifulSoup(text, 'lxml').get_text() \n",
    "    text = text.lower()\n",
    "    words_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    return(words_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.title = combined_data.title.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_data.to_csv('./datasets/zelda-bitcoin.csv', index=False, index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_data['title']\n",
    "y = combined_data['subreddit'].map(lambda x: 1 if x == 'Bitcoin' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.664466\n",
       "0    0.335534\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_model(vectorizer, model):    \n",
    "    vect_dict = {\n",
    "        'count' : CountVectorizer(), \n",
    "        'td' : TfidfVectorizer()\n",
    "    }\n",
    "\n",
    "    model_dict = {\n",
    "        'bayes' : MultinomialNB(), \n",
    "    }\n",
    "\n",
    "    steps = [\n",
    "        ('vectorize', vect_dict[vectorizer]),\n",
    "        ('model', model_dict[model])\n",
    "    ]\n",
    "\n",
    "    return Pipeline(steps = steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing different vectorizors with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_count_model = vect_model('count', 'bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorize', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preproc...zer=None, vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vectorize__stop_words': ['english', None], 'vectorize__ngram_range': [(1, 1), (1, 2)], 'vectorize__max_features': [5000, 10000, None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'vectorize__stop_words': ['english', None],\n",
    "    'vectorize__ngram_range': [(1,1), (1,2)],\n",
    "    'vectorize__max_features' : [5000, 10000, None]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(bayes_count_model, pipe_params, n_jobs=1, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorize__max_features': None,\n",
       " 'vectorize__ngram_range': (1, 2),\n",
       " 'vectorize__stop_words': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876660341555977"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571320182094082"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with tifidf\n",
    "bayes_td_model = vect_model('td', 'bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-2)]: Done 144 out of 144 | elapsed:   11.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tru...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-2,\n",
       "       param_grid={'vectorize__stop_words': ['english', None], 'vectorize__ngram_range': [(1, 1), (1, 2)], 'vectorize__max_features': [5000, 10000, None], 'vectorize__lowercase': [True, False], 'vectorize__norm': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'vectorize__stop_words': ['english', None],\n",
    "    'vectorize__ngram_range': [(1,1), (1,2)],\n",
    "    'vectorize__max_features' : [5000, 10000, None],\n",
    "    'vectorize__lowercase' : [True, False],\n",
    "    'vectorize__norm' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(bayes_td_model, pipe_params, n_jobs=-2, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorize__lowercase': True,\n",
       " 'vectorize__max_features': 5000,\n",
       " 'vectorize__ngram_range': (1, 1),\n",
       " 'vectorize__norm': 'l2',\n",
       " 'vectorize__stop_words': 'english'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9671726755218216"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9438543247344461"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high accuarcy on train and test data for both naive bayes and logistic regression, we can conclude that this methodology works well given two subreddits with completly unrelated content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
