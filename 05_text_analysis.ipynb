{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nixfu</td>\n",
       "      <td>no completed deals   not doing business   he was looking into it but never did it    duh</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crypulous</td>\n",
       "      <td>trump has been amazing on a lot of things  but on the wall i agree with ann  i also think trump responds to certain critics  so coulter is actually doing him a favor by keeping the pressure on  you can support someone without agreeing with everything they do  no wall would be very bad for trump s legacy  a wall and an end to anchor babies are key issues for a lot of voters</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dueler312</td>\n",
       "      <td>actually fox news did show it</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soberlight</td>\n",
       "      <td>the excuse about being concerned about their response is pretty thin considering they re trampling over rule of law as it is   we don t really have much of a tomorrow when it comes to the whoring of justice     x   b     x   b</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enterthewalrus</td>\n",
       "      <td>well arizona did not get martha mcsally but it looks like the did trade up to get sinema</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  \\\n",
       "0           nixfu   \n",
       "1       Crypulous   \n",
       "2       Dueler312   \n",
       "3      soberlight   \n",
       "4  enterthewalrus   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                       body  \\\n",
       "0                                                                                                                                                                                                                                                                                               no completed deals   not doing business   he was looking into it but never did it    duh      \n",
       "1  trump has been amazing on a lot of things  but on the wall i agree with ann  i also think trump responds to certain critics  so coulter is actually doing him a favor by keeping the pressure on  you can support someone without agreeing with everything they do  no wall would be very bad for trump s legacy  a wall and an end to anchor babies are key issues for a lot of voters    \n",
       "2                                                                                                                                                                                                                                                                                                                                                           actually fox news did show it     \n",
       "3                                                                                                                                                       the excuse about being concerned about their response is pretty thin considering they re trampling over rule of law as it is   we don t really have much of a tomorrow when it comes to the whoring of justice     x   b     x   b    \n",
       "4                                                                                                                                                                                                                                                                                                 well arizona did not get martha mcsally but it looks like the did trade up to get sinema    \n",
       "\n",
       "         date  score   subreddit  \n",
       "0  2018-11-30      1  The_Donald  \n",
       "1  2018-11-30      1  The_Donald  \n",
       "2  2018-11-30      1  The_Donald  \n",
       "3  2018-11-30      1  The_Donald  \n",
       "4  2018-11-30      1  The_Donald  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('./datasets/cleaned_donald_chappo_text.csv', index_col=0)\n",
    "train_text.dropna(inplace=True)\n",
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57710, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_text['body']\n",
    "y = train_text['subreddit'].map(lambda x: 1 if x == 'The_Donald' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.511159\n",
       "0    0.488841\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Text with Logistic Regression (no stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dataframe(text, n_features, sw=None):\n",
    "    count = CountVectorizer(stop_words=sw, max_features=n_features)\n",
    "    text = count.fit_transform(text)\n",
    "    text = text.toarray()\n",
    "    return pd.DataFrame(text, columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_vect = text_dataframe(X, n_features=40000, sw='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_vect, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8540807485704384"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.score(X_vect, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictonary with key values pairs that correspond to the odds and associated word as calculated by the logistic regressoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {}\n",
    "\n",
    "for i in zip(np.exp(logit.coef_[0]),X_vect.columns):\n",
    "    coef_dict[float(i[0])] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the largest 15 odds, these correlate strongly text from The Donald\n",
    "donald_top_ten = pd.Series(np.exp(logit.coef_[0])).sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the smallest 15 odds, these correlate strongly text from ChapoTrapHouse\n",
    "chapo_top_ten = pd.Series(np.exp(logit.coef_[0])).sort_values(ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 15 words that correlate with The Donald and their associated odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pede --- 35.64945049729865\n",
      "pedes --- 18.575974212978817\n",
      "kek --- 18.264577438722885\n",
      "seth --- 11.890973949991421\n",
      "geotus --- 9.907162200279926\n",
      "redacted --- 8.390915901586679\n",
      "swamp --- 8.319730573413143\n",
      "bigot --- 7.02894685563287\n",
      "cnn --- 6.88520345436\n",
      "msm --- 6.775028675390836\n",
      "invaders --- 6.292425256350223\n",
      "fraud --- 6.150562659472404\n",
      "reeeeeee --- 5.938297442591888\n",
      "bongino --- 5.92558642226683\n",
      "doj --- 5.828860889645256\n"
     ]
    }
   ],
   "source": [
    "for i in donald_top_ten:\n",
    "    print(coef_dict[i], '---', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 15 words that correlate with ChapoTrapHouse and their associated odds (relative to being in The Donald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hog --- 0.016530407379071032\n",
      "chapo --- 0.017758700870684076\n",
      "chud --- 0.020228481038927222\n",
      "chuds --- 0.024050645155193093\n",
      "hellworld --- 0.03845439688377442\n",
      "praxis --- 0.04441243122423325\n",
      "virgil --- 0.05910299916998219\n",
      "comrade --- 0.06876201661473547\n",
      "volcel --- 0.07055586846998575\n",
      "creator --- 0.07689137885955226\n",
      "felix --- 0.07865962697434863\n",
      "neoliberal --- 0.07906274396118222\n",
      "centrists --- 0.08220656490271964\n",
      "hogs --- 0.08747933681358229\n",
      "reactionary --- 0.09358096760763611\n"
     ]
    }
   ],
   "source": [
    "for i in chapo_top_ten:\n",
    "    print(coef_dict[i], '---', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Text with Logistic Regression (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_vect = text_dataframe(X, n_features=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_vect, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {}\n",
    "\n",
    "for i in zip(np.exp(logit.coef_[0]),X_vect.columns):\n",
    "    coef_dict[float(i[0])] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_top_ten = pd.Series(np.exp(logit.coef_[0])).sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapo_top_ten = pd.Series(np.exp(logit.coef_[0])).sort_values(ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.47783685968139 --- pede\n",
      "19.782647791562685 --- kek\n",
      "17.045163424797746 --- pedes\n",
      "11.190944183111428 --- seth\n",
      "9.958451451189601 --- geotus\n",
      "8.231329461798067 --- swamp\n",
      "7.422725562394931 --- redacted\n",
      "7.120373659441393 --- bigot\n",
      "6.865087504485365 --- rsbn\n",
      "6.775433913468901 --- cnn\n",
      "6.472031307695681 --- bongino\n",
      "6.344885802804554 --- msm\n",
      "6.237600762235194 --- invaders\n",
      "6.230367424647058 --- reeeeeee\n",
      "6.064776327180565 --- fraud\n"
     ]
    }
   ],
   "source": [
    "for i in donald_top_ten:\n",
    "    print(i, '---', coef_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01787706445446757 --- chapo\n",
      "0.017925598970587285 --- hog\n",
      "0.019751544548353858 --- chud\n",
      "0.02326630226393725 --- chuds\n",
      "0.04075526840626377 --- hellworld\n",
      "0.0446548971525446 --- praxis\n",
      "0.06209126235926216 --- virgil\n",
      "0.06805762319936069 --- comrade\n",
      "0.07133231125673185 --- neoliberal\n",
      "0.0760120969740782 --- volcel\n",
      "0.07829353870981363 --- creator\n",
      "0.08057010209022371 --- felix\n",
      "0.08824406940995792 --- centrists\n",
      "0.08891462278970465 --- hogs\n",
      "0.09107684503462962 --- tankies\n"
     ]
    }
   ],
   "source": [
    "for i in chapo_top_ten:\n",
    "    print(i, '---', coef_dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the most frequent words in both subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking up the combined data by subreddit for simplicity\n",
    "the_donald = train_text.loc[train_text['subreddit'] == 'The_Donald']\n",
    "chapo = train_text.loc[train_text['subreddit'] == 'ChapoTrapHouse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshillbot_mask = chapo.body.map(lambda x: True if 'SnapshillBot' in x else False)\n",
    "chapo = chapo.loc[~snapshillbot_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29499, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_donald.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28211, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to return a dataframe with the total count of each word in the text\n",
    "def word_frequency(text, sw=None, ngrams=(1,1)):\n",
    "    count = CountVectorizer(stop_words=sw, ngram_range=ngrams)\n",
    "    text = count.fit_transform(text)\n",
    "    sums = text.toarray().sum(axis=0)\n",
    "    return pd.DataFrame([sums], columns=count.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapo_no_s_words = word_frequency(chapo.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>26917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>15976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>13198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>12916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>10253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>9846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>8264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>8086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>5592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "the   26917\n",
       "to    15976\n",
       "and   13198\n",
       "of    12916\n",
       "that  10253\n",
       "is    10090\n",
       "it     9846\n",
       "you    8264\n",
       "in     8086\n",
       "for    5592"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapo_no_s_words.sort_values(0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapo_s_words = word_frequency(chapo['body'], sw='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>3507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>3240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don</th>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shit</th>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "like    3507\n",
       "people  3240\n",
       "just    3131\n",
       "don     1978\n",
       "think   1658\n",
       "good    1439\n",
       "right   1293\n",
       "shit    1247\n",
       "really  1155\n",
       "know    1150"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapo_s_words.sort_values(0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_no_s_words = word_frequency(the_donald['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>27364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>16895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>14035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>11212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>10389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>10050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>8481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>7172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "the   27364\n",
       "to    16895\n",
       "and   14035\n",
       "of    11212\n",
       "is    10389\n",
       "it    10050\n",
       "that   9504\n",
       "in     8481\n",
       "you    7294\n",
       "they   7172"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donald_no_s_words.sort_values(0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_s_words = word_frequency(the_donald['body'], sw='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>2940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>2832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don</th>\n",
       "      <td>2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "like    2940\n",
       "just    2832\n",
       "people  2525\n",
       "don     2141\n",
       "trump   1824\n",
       "think   1352\n",
       "know    1302\n",
       "time    1159\n",
       "right   1119\n",
       "good    1043"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donald_s_words.sort_values(0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the number of unique words in the top 1,000 words for each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 1000 most common words in the donald\n",
    "donald_top_1000 = list(donald_s_words.sort_values(0, ascending=False).head(1000).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 1000 most common words in chapo trap house\n",
    "chapo_top_1000 = list(chapo_s_words.sort_values(0, ascending=False).head(1000).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the number of these words that are unique to the donald\n",
    "donald_specific = []\n",
    "for word in donald_top_1000:\n",
    "    if word not in chapo_top_1000:\n",
    "        donald_specific.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(donald_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the number of these words that are unique to the chapo trap house (this should be the same)\n",
    "donald_specific = []\n",
    "chapo_specific = []\n",
    "for word in chapo_top_1000:\n",
    "    if word not in donald_top_1000:\n",
    "        chapo_specific.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chapo_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, repeating the process with the Zelda and Bitcoin comments to compare unique word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13159, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data = pd.read_csv('./datasets/zelda-bitcoin.csv', index_col=0)\n",
    "combined_data.dropna(inplace=True)\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda = combined_data.loc[combined_data['subreddit'] == 'zelda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zelda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin = combined_data.loc[combined_data['subreddit'] == 'Bitcoin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8739, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_words = word_frequency(zelda['title'], sw='english')\n",
    "bitcoin_words = word_frequency(bitcoin.title, sw='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_top_1000 = list(zelda_words.sort_values(0, ascending=False).head(1000).index)\n",
    "bitcoin_top_1000 = list(bitcoin_words.sort_values(0, ascending=False).head(1000).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda_specific = []\n",
    "for word in zelda_top_1000:\n",
    "    if word not in bitcoin_top_1000:\n",
    "        zelda_specific.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zelda_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_specific = []\n",
    "for word in bitcoin_top_1000:\n",
    "    if word not in zelda_top_1000:\n",
    "        bitcoin_specific.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bitcoin_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly more unique words for the Zelda and Bitcoin subreddits, this makes sense and helps to explain why the classification of these subreddits had a significantly higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that returns the shared words out of the n most common words\n",
    "def common_words(num):\n",
    "    donald = list(donald_s_words.sort_values(0, ascending=False).head(num).index)\n",
    "    chapo = list(chapo_s_words.sort_values(0, ascending=False).head(num).index)\n",
    "\n",
    "    common = []\n",
    "    for word in donald:\n",
    "        if word in chapo:\n",
    "            common.append(word)\n",
    "            \n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that returns the unique words for each subreddit out of the n most common words\n",
    "def seperate_words(text_1, text_2, num):\n",
    "    text_1 = list(text_1.sort_values(0, ascending=False).head(num).index)\n",
    "    text_2 = list(text_2.sort_values(0, ascending=False).head(num).index)\n",
    "\n",
    "    text_1_list = []\n",
    "    for word in text_1:\n",
    "        if word not in text_2:\n",
    "            text_1_list.append(word)\n",
    "            \n",
    "    text_2_list = []\n",
    "    for word in text_2:\n",
    "        if word not in text_1:\n",
    "            text_2_list.append(word)\n",
    "            \n",
    "    return text_1_list, text_2_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top phrases in each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_bigrams = word_frequency(the_donald['body'], ngrams=(2,2))\n",
    "chapo_bigrams = word_frequency(chapo.body, ngrams=(2,2))\n",
    "\n",
    "unique_donald_bigrams, unique_chapo_bigrams = seperate_words(donald_bigrams, chapo_bigrams, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump is',\n",
       " 'president trump',\n",
       " 'the president',\n",
       " 'the wall',\n",
       " 'fake news',\n",
       " 'if if',\n",
       " 'the law',\n",
       " 'to win',\n",
       " 'the border',\n",
       " 'orange man',\n",
       " 'our country',\n",
       " 'we will',\n",
       " 'they need',\n",
       " 'did not',\n",
       " 'allowed to']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_donald_bigrams[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right wing',\n",
       " 'working class',\n",
       " 'go on',\n",
       " 'was created',\n",
       " 'created by',\n",
       " 'is pretty',\n",
       " 'holy shit',\n",
       " 'this message',\n",
       " 'by bot',\n",
       " 'message was',\n",
       " 'contact creator',\n",
       " 'bot contact',\n",
       " 'your hog',\n",
       " 'pretty much',\n",
       " 'on chapo']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chapo_bigrams[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_trigrams = word_frequency(the_donald['body'], ngrams=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapo_trigrams = word_frequency(chapo.body, ngrams=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_donald_trigrams, unique_chapo_trigrams = seperate_words(donald_trigrams, chapo_trigrams, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if if if',\n",
       " 'we have the',\n",
       " 'orange man bad',\n",
       " 'the white house',\n",
       " 'the deep state',\n",
       " 'in the house',\n",
       " 'build the wall',\n",
       " 'have the best',\n",
       " 'and they are',\n",
       " 'the love of',\n",
       " 'for the love',\n",
       " 'all over the',\n",
       " 'we don need',\n",
       " 'love of god',\n",
       " 'women and children']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_donald_trigrams[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was created by',\n",
       " 'message was created',\n",
       " 'bot contact creator',\n",
       " 'created by bot',\n",
       " 'this message was',\n",
       " 'by bot contact',\n",
       " 'go on chapo',\n",
       " 'the working class',\n",
       " 'to own the',\n",
       " 'the volcel police',\n",
       " 'post your hog',\n",
       " 'the guy who',\n",
       " 'show us your',\n",
       " 'you re just',\n",
       " 'no nut november']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chapo_trigrams[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>BiglyPepe</td>\n",
       "      <td>wow  you should run for president seems like you have it all figured out  orange man bad</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>DoYouBelieveInMAGA</td>\n",
       "      <td>x   b   orange man bad    x   b</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>CaptainFrosty88</td>\n",
       "      <td>the year is      general obama  now a genetically modified robot  and colonel clinton have just entered the   th year of the great orange man bad conflict  the racist pig general trump has lead the right to march upon our lands  california is the last remaining state  our munitions are down to   musket per   men  we have to share  as we outlawed every gun except muskets to be safe  meanwhile the right are cheating and using full autos  the bastards  i fear this day will be my last</td>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>gordo7054</td>\n",
       "      <td>while downing s discussions with the president s team violated no laws     it could possibly help trump which should be illegal because orange man bad</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>cl1ft</td>\n",
       "      <td>orange man bad    someone compliment orange man  they re bad    compliment must be false  bad</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>1</td>\n",
       "      <td>The_Donald</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  \\\n",
       "175            BiglyPepe   \n",
       "182   DoYouBelieveInMAGA   \n",
       "745      CaptainFrosty88   \n",
       "2009           gordo7054   \n",
       "2169               cl1ft   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body  \\\n",
       "175                                                                                                                                                                                                                                                                                                                                                                                                               wow  you should run for president seems like you have it all figured out  orange man bad     \n",
       "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                   x   b   orange man bad    x   b          \n",
       "745   the year is      general obama  now a genetically modified robot  and colonel clinton have just entered the   th year of the great orange man bad conflict  the racist pig general trump has lead the right to march upon our lands  california is the last remaining state  our munitions are down to   musket per   men  we have to share  as we outlawed every gun except muskets to be safe  meanwhile the right are cheating and using full autos  the bastards  i fear this day will be my last    \n",
       "2009                                                                                                                                                                                                                                                                                                                                                 while downing s discussions with the president s team violated no laws     it could possibly help trump which should be illegal because orange man bad    \n",
       "2169                                                                                                                                                                                                                                                                                                                                                                                                          orange man bad    someone compliment orange man  they re bad    compliment must be false  bad    \n",
       "\n",
       "            date  score   subreddit  \n",
       "175   2018-11-30      1  The_Donald  \n",
       "182   2018-11-30      1  The_Donald  \n",
       "745   2018-11-29      1  The_Donald  \n",
       "2009  2018-11-28      1  The_Donald  \n",
       "2169  2018-11-28      1  The_Donald  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at how the phrase orange man bad is used\n",
    "orange_man_mask = the_donald.body.map(lambda x: True if 'orange man bad' in x else False)\n",
    "the_donald.loc[orange_man_mask, :].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling with custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_10 = common_words(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union(common_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_text.body\n",
    "y = train_text.subreddit.map(lambda x: 1 if x == 'The_Donald' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words=common_10)\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count.fit_transform(X_train)\n",
    "X_test = count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51939, 39942)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8165155278307245"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7336683417085427"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_100 = common_words(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union(common_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_text.body\n",
    "y = train_text.subreddit.map(lambda x: 1 if x == 'The_Donald' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words=common_100)\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count.fit_transform(X_train)\n",
    "X_test = count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51939, 39874)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8158031536995322"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7282966556922543"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight Decrease in Performance - Removing common words does not seem to improve the score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
